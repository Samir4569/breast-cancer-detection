# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vaIjU7MVIGqy0fy5uVyF6fkDuhhr4J6k
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
#

from google.colab import files
uploaded = files.upload()
df = pd.read_csv('data.csv')

df.head(7)

df.info()

df.shape

df.isnull().any()

df.dropna(axis=1,inplace=True)

df.isnull().sum()

df.shape

df['diagnosis'].value_counts()

sns.countplot(df['diagnosis'])

df['diagnosis'] = df['diagnosis'].map({'M':1,'B':0})

df['diagnosis'].value_counts()

sns.pairplot(df.iloc[:,1:6], hue='diagnosis')
plt.show()

df.iloc[:,1:12].corr()

plt.figure(figsize=(7,6))
sns.heatmap(df.iloc[:,1:12].corr(), annot=True)
plt.show()

X = df.iloc[:,2:].values
y = df.iloc[:,1].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

def models(X_train, y_train):
  from sklearn.linear_model import LogisticRegression
  log = LogisticRegression(random_state=0)
  log.fit(X_train, y_train)

  from sklearn.neighbors import KNeighborsClassifier
  knn = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)
  knn.fit(X_train, y_train)

  from sklearn.tree import DecisionTreeClassifier
  tree = DecisionTreeClassifier(criterion='entropy', random_state=0)
  tree.fit(X_train, y_train)

  from sklearn.ensemble import RandomForestClassifier
  forest = RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)
  forest.fit(X_train, y_train)

  print('[0]Logistic Regression Training Accuracy', log.score(X_train, y_train))
  print('[1]K Nearest Neighbor Training Accuracy', knn.score(X_train, y_train))
  print('[2]Decision Tree Training Accuracy', tree.score(X_train, y_train))
  print('[3]Random Forest Training Accuracy', forest.score(X_train, y_train))

  return log, knn, tree, forest

log, knn, tree, forest = models(X_train, y_train)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, forest.predict(X_test))
cm

print('accuracy :', (cm[0][0] + cm[1][1]) / (cm[0][0] + cm[0][1] + cm[1][0] + cm[1][1]))

cm = confusion_matrix(y_test, log.predict(X_test))
cm

print('accuracy :', (cm[0][0] + cm[1][1]) / (cm[0][0] + cm[0][1] + cm[1][0] + cm[1][1]))

cm = confusion_matrix(y_test, tree.predict(X_test))
cm

print('accuracy :', (cm[0][0] + cm[1][1]) / (cm[0][0] + cm[0][1] + cm[1][0] + cm[1][1]))

cm = confusion_matrix(y_test, knn.predict(X_test))
cm

print('accuracy :', (cm[0][0] + cm[1][1]) / (cm[0][0] + cm[0][1] + cm[1][0] + cm[1][1]))

from sklearn.metrics import classification_report
print(classification_report(y_test, forest.predict(X_test)))

print(classification_report(y_test, knn.predict(X_test)))

print(classification_report(y_test, tree.predict(X_test)))

print(classification_report(y_test, log.predict(X_test)))

from sklearn.metrics import classification_report, accuracy_score

# Evaluate on Training Data
y_train_pred = forest.predict(X_train)
train_accuracy = accuracy_score(y_train, y_train_pred)
print(f'Training Accuracy: {train_accuracy * 100:.2f}%')
print('Training Classification Report:')
print(classification_report(y_train, y_train_pred))





